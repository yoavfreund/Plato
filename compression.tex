% Model accuracy definition in modelbaseddbs
% encoder - decoder

\section{Opportunities in Data Compression}
\label{sec:compression}

Data compression and statistical models are closely related
concept. Good statistical models allow Plato to achieve high
compression rates. On the other hand, high compression rates indicate
that plato has correctly identified significant statistical patterns
in the data, and those patterns are potentially useful for the
analyst.

We now elaborate on this interdependence of models and compression and
on the way they are used in Plato.

\subsection{Lossless compression}
A lossless compression method is one which can reconstruct the
original measurements without error. Popular lossless compression
methods are {\tt gzip} and {\tt compress}. A lossless compression
method corresponds to a statistical model over the data and the
expected compression ratio is determined by the distance between
statistical model and the empirical distribution of the data (as
measured by the the Kullback-Leibler-divergence). 

Unfortunately, lossless compression methods usually achieve a
compression ratio of around of around 2 to 4. Which corresponds to the
fact that it accurately captures both the true state of the world (the signal)
and the many random influences on the measurements (the noise). 
Lossless compression does not distinguish between the two.

\newcommand{\vx}{\mathbf{x}}
\newcommand{\hx}{\hat{x}}
\newcommand{\vhx}{\hat{\mathbf{x}}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vr}{\mathbf{r}}

\subsection{Lossy compression}
In order to reach higher compression ratios than lossless
compression. It is common to use {\em Lossy compression}~\cite{CompressionBook}.

Lossy compression is based on the assumption that the data to be
compressed is a point in Euclidean space. From this point onward we
assume that the measurements arriving from a (single) sensor is
represented as a sequence of real values: $\vx=(x_1,x_2,\ldots,x_t)$.
We denote the reconstruction of the sequence from the compressed
version by $\vhx=(\hx_1,\hx_2,\ldots,\hx_t)$. As we are using lossy
compression $\vhx$ is not equal to $\vx$. The difference between the
two sequences is called the {\em residual} which we denote by $\vr =
\vx -\vhx$.  We require that the residual is small. The measure of
the size of the residual is called the {\em distortion}. We use two of
the most popular distortion measures.
\begin{itemize}
\item By far, the most common measure of distortion is the $L_2$ error
  also called {\em root-mean-square} error or RMS:
\[
\mbox{RMS}\left(\vhx,\vx \right)
\doteq \sqrt{\frac{\sum_{i=1}^t r_i^2}{t}}
\]
\item If the errors are usually small and only rarly very large,
the RMS is misleadingly large. A better error measure in such
situations is the fraction of times the error is larger
than some threshold:
\[
\mbox{err}_{\epsilon}\left(\vhx,\vx \right)
\doteq \frac{\sum_{i=1}^t \mathbf{1}\left(|r_i|>\epsilon \right)}{t}
\]
\end{itemize}
Lossy compression methods such as {\tt jpeg2000} for images and {\tt
  mp3} for audio can achieve compression ratios of 100 or more
with no perceptible degredation in quality.

\section{Additive models}
Some of the most commonly used models used for lossy compression are
additive with respect to the RMS distortion measure. This set of
models includes FFT, Wavelets, and SVD. What we mean here by
additivity is that any model is a sum of components: $\vhx =
\sum_{i=1}^k \vc_i$;m that the best model for $k_2>k_1$ shares the
first $k_1$ components with the best model using $k_1$ components; and
that the reduction in the distortion is largest for the first component
and decreases monotonically A $k$ increases.

Additive models lend themselves to an incremental compression
scheme. Starting by setting the residual to be to original
measurements $\vr_0 \leftarrow \vx$, repeat the following steps for
$i=1,2,\ldots$
\begin{itemize}
\item Find the vector $\vc_i$ that minimizes the RMS error: 
$\mbox{RMS}\left(\vc,\vr \right)$
\item Subtract the identified component from the residual: $\vr_i =
  \vr_{i-1}-\vc_i$
\end{itemize}
This iterative process is repeated until $\vr=0$. At that point we
have a lossless compression of the original sequence. However, this
sequence is organized in such a way that one can build the
reconstruction of the original sequence incrementally, quickly getting
a reasonable approximation and then improving that approximation to
the desired level. (YK, is that what you were looking for?)

\section{Model Selection}
\label{sec:Model Selection}
The practice of data compression~\cite{Sayood12}, shows that most
real-world sensor generated sequences can be compressed reasonably
well using one of a small set of statistical models such as ARMA
models, Fourier and wavelet-based models and Singular Value
Decomposition models (SVD or PCA). 

The key responsibility of the data administrator is to choose a set of
viable models, which we refer to as {\em Codecs}. Plato will then use
machine learning algorithms to tune the parameters of the different
Codecs so that they best fit the data. It will then compare the
performance of the different codecs in terms of compression and
distortion and choose the model that performs best.

When the data to be compressed arrives from a large number of
proximate sensors, there are potentially large benefits from
compressing them together using a predictive statistical model such as
ARMA. The space of models that capture all possible dependencies
between the sensor readings is extremely large and hard to learn. The
role of the data administrator in this context is to restrict the set
of possible interactions to make the learning problem easier. In the
example of modeling the temperatures of rooms in a building the model
might be restricted to considering the dependence between the outside
temperature and that of the rooms and in addition, the dependence
between neighboring rooms. A model that identifies the stronger
dependencies within these restrictions is useful both for data
compression, as a model of the physical reality (which rooms are
better insulated than others) and as a basis for noise reduction.

\subsection{Noise reduction}
Lossy compression methods invariably loose some of the original
data. However, it is often the case that the result is a
reconstruction that is {\em closer} to the true underlying real-world
values than the original measurements. How is that possible?

The answer was hinted at in the previous section. Noise reduction is
possible when a good model of the dependencies between different
sensors is available. In such situations the compression and
decompression of the data shifts the data from the measured vector
towards the manifolds that is consistent with the physical and
statistical properties of the data. The result is similar to that of
{\em smoothing} the data, with the important difference that the
smoothing is based on the statistical model that was inferred from the data.

\iffalse
This is possible when the model used for compression captures some
inherent regularity in the data. Consider a temperature guage in a
room that is taking a measurement ten times per second. Suppose also
that each measurement is the sum of the true temperature and gaussian
noise with a standard deviation of one degree.  It is clear that
replacing blocks of 10 consecutive measurements by their average is
lossy in terms of the original signal. However, at the same time, the
averaged sequence is closer to the true measurements. This noise
reduction relies of the assumption that room temperature rarely
changes significantly within a tenth of a second. Therefor any rapid
variations in the measured temperature is likely to be an artifact of
the sensor rather than anything real.
\fi

The goal of noise reduction gives rise to a different way of measuring
the quality of a compression scheme. Instead of considering the {\em
  amplitude} of the residual using RMS or $\mbox{err}_{\epsilon}$ we
can consider whether or not the residual is white noise. If the models
that we consider are linear we can measure how similar the residual is
to white noise by considering the auto-correlation function for each
residual and the cross-correlation between each pair of residuals. We
expect the auto-correlation to consist of a single delta-function at
zero and we expect the cross-correlation functions to be close to zero
everywhere.

\subsubsection{Model Maintenance}

When considering a database system that processes data streams over
long periods of time it is necessary to consider how to maintain the
models over time. We divide this problem into two parts, depending on
whether or not the data source is stationary.

When the data source is stationary, efficient model maintenance is
closely related to the subjects of sufficient statistics and or data
synopses or sketches~\cite{}. In any of these formulation the goal is
to define a small data structure that stores the information necessary
to define the model. We plan to test the methods proposed in the
literature and, where necessary, develop new ones.

When the data source is not stationary, we need our model to track the
distribution of the data. This causes an unavoidable tradeoff between
accuracy and adaptivity. In order to adapt quickly the model has to
ignore data from the distant past. At the same time, ignoring data
from the distant past limit the accuracy at which the model parameters
can be estimated. Calibrating the algorithm to consider the optimal
window into the past has been extensively studied in online learning
theory~\cite{Cesa-Bianchi}, in particular
in~\cite{HerbsterWarmuth,Specialists,Warmuth bosquet}. Plato will use
these online learning methods for efficient mode maintenance when the
data distribution varies over time.

\eat{
\reminder{skip this subsection. may become irrelevant due to the increasing depth representation discussed in the query processing}
The model administrator can choose more or less compressed model alternatives. The choice presents a speed-accuracy trade-off: Noise-reducted model alternatives will enable precise query answers but at the cost of speed, since their representations will be relatively large. In contrast, 
lossy model alternatives will lead to very fast queries but at the cost of query answer accuracy. Plato will provide a {\em model administrator assistant} module that semiautomates the process of choosing the appropriate model representation by solving the following problems.

% query answering using a model alternative
% what is the accuracy damage

\paragraph{Choosing a model alternative given a query}
In the simplest setting, the assistant is given
\begin{enumerate} 
%
\item data measurements.
%
\item a model generator that can produce a noise-reducted model $f_{nr}$ of the measurements
%
\item a lossy model generator and candidate loss parameters. For example, the assistant may given the \texttt{fourier\_rms} model generator with candidate RMS error (loss parameter) 0.01, 0.02, 0.03, ..., 0.20. In principle, each setting $e_i$ of the loss parameter leads to another model $f_i$. Furthermore, the representation size of $f_i$ is larger than the representation size of $f_{i+1}$ and all of them are smaller than $f_{nr}$. Let us call 
%
\item a single query $Q$ that uses a model $f$ and a specification of the desired accuracy of the result.
%
\end{enumerate}
}

\subsection{Adjusting to filtering needs}

\reminder{To Yannis K: Add discussion explaining which models are good for pushing selections down}
