% Model accuracy definition in modelbaseddbs
% encoder - decoder

\section{Inferring Models and Representing them Compactly}
\label{sec:compression}
The practice of data compression~\cite{Sayood12} shows that most
real-world sensor generated sequences can be captured reasonably
well using one of a small set of statistical models such as ARMA
models, Fourier and wavelet-based models and Singular Value
Decomposition models (SVD or PCA). 
Data compression and statistical models are closely related
concepts. Good statistical models allow Plato to achieve high
compression rates. Vice versa, high compression rates indicate
that Plato has correctly identified significant statistical patterns
in the data and has got rid of the noise. The following discussion
explains how the model administrator chooses the appropriate models
and makes the case for providing additive, reduced-noise models.

\reminder{To YF and YP: Please check paragraph below. I removed the codecs and presented instead learning algorithms creating models and corresponding storage representations.}
The key responsibility of the model administrator (see Figure~\ref{fig-architecture.pdf}) is to choose a set of learning algorithms to create corresponding models over the measurements tables. Each learning algorithm creates a model that fits the data and (in case the administrator has selected to materialize it) stores it using a learning algorithm-specific storage representation. For instance, as shown in Figure~\ref{fig-architecture.pdf}, a possible storage representation of a model created by the ARMA learning algorithm is the set of the ARMA coefficients. Once the chosen models have been created, the administrator can compare them in terms of compression and distortion and choose the one that performs best.

%\reminder{to YF and YK: The term codecs makes below its first appearance.}
%The key responsibility of the model administrator (see Figure~\ref{fig:db-driven-arch}) is to choose a set of viable models, which we refer to as {\em codecs}. Plato will then use machine learning algorithms to tune the parameters of the different codecs so that they best fit the data. The administrator will then compare the performance of the different codecs in terms of compression and distortion and choose the model that performs best.

\reminder{To YF and YK: There is still a slight terminology mismatch, as below we refer to the storage representation of models, while we really mean the storage representation generated by a particular learning algorithm.}
There are multiple models and respective compressions that are applicable, as explained next. In the interest of comparisons to prior work, we initiate the discussion with a presentation of lossless compression and then continue with models involving lossy compression. In practice, Plato will use additive and reduced-noise models only. The underlying logic is that we aim for minimum query processing time and high quality models, while storage is a secondary only consideration.


\subsection{Lossless compression}
\label{sec:lossless}

A lossless compression method is one which can reconstruct the
original measurements without error. Popular lossless compression
methodss are {\tt gzip} and {\tt compress}. A lossless compression
method corresponds to a statistical model over the data and the
expected compression ratio is determined by the distance between
the statistical model and the empirical distribution of the data (as
measured by the the Kullback-Leibler divergence). 

Unfortunately, lossless compression methods usually achieve a
compression ratio of around 2 to 4. This corresponds to the
fact that lossless compression accurately captures both the true state of the world (the signal)
and the many random influences on the measurements (the noise). 
Lossless compression does not distinguish between the two.

\newcommand{\vx}{\mathbf{x}}
\newcommand{\hx}{\hat{x}}
\newcommand{\vhx}{\hat{\mathbf{x}}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vr}{\mathbf{r}}

\subsection{Lossy compression}
In order to reach higher compression ratios than lossless
compression, it is common to use {\em lossy compression}~\cite{CompressionBook}.

Lossy compression is based on the assumption that the data to be
compressed is a point in Euclidean space. From this point onward we
assume that the measurements arriving from a (single) sensor are
represented as a sequence of real values $\vx=(x_1,x_2,\ldots,x_t)$.
We denote the reconstruction of the sequence from the compressed
version by $\vhx=(\hx_1,\hx_2,\ldots,\hx_t)$. As we are using lossy
compression $\vhx$ is not equal to $\vx$. The difference between the
two sequences is called the {\em residual} which we denote by $\vr =
\vx -\vhx$.  We require that the residual is small. 
\reminder{to YF: Just small? We really require that it is noise.
}
The measure of
the size of the residual is called the {\em distortion}. We use two of
the most popular distortion measures.
\begin{itemize}
\item By far, the most common measure of distortion is the $L_2$ error
  also called {\em root-mean-square} error or RMS:
\[
\mbox{RMS}\left(\vhx,\vx \right)
\doteq \sqrt{\frac{\sum_{i=1}^t r_i^2}{t}}
\]
\item If the errors are usually small and only rarely very large,
the RMS is misleadingly large. A better error measure in such
situations is the fraction of times the error is larger
than some threshold:
\[
\mbox{err}_{\epsilon}\left(\vhx,\vx \right)
\doteq \frac{\sum_{i=1}^t \mathbf{1}\left(|r_i|>\epsilon \right)}{t}
\]
\end{itemize}
\reminder{to YF: Instead of audio and image examples, please provide a signal processing case where wavelets/FFT, SVD, ARMA provide good compression. I made a mention to Matt's efforts.
}
Lossy compression methods such as {\tt jpeg2000} for images and {\tt
  mp3} for audio can achieve compression ratios of 100 or more
with no perceptible degradation in quality.

\subsection{Additive models}
\label{sec:additive}
Some of the most commonly used models for lossy compression are
additive with respect to the RMS distortion measure. This set of
models includes FFT, Wavelets, and SVD. What we mean here by
additivity is that any model is a sum of components: $\vhx =
\sum_{i=1}^k \vc_i$; that the best model for $k_2>k_1$ shares the
first $k_1$ components with the best model using $k_1$ components; and
that the reduction in the distortion is largest for the first component
and decreases monotonically as $k$ increases.

Additive models lend themselves to an incremental compression
scheme. Starting by setting the residual to the original
measurements $\vr_0 \leftarrow \vx$, repeat the following steps for
$i=1,2,\ldots$
\begin{itemize}
\item Find the vector $\vc_i$ that minimizes the RMS error: 
$\mbox{RMS}\left(\vc,\vr \right)$
\item Subtract the identified component from the residual: $\vr_i =
  \vr_{i-1}-\vc_i$
\end{itemize}
This iterative process is repeated until $\vr=0$. At that point we
have a lossless compression of the original sequence. However, this
sequence is organized in such a way that one can build the
reconstruction of the original sequence incrementally, quickly getting
a reasonable approximation and then improving that approximation to
the desired level. 

\subsection{Reduced-noise models}
\label{sec:reduced-noise}

Lossy models invariably loose some of the original
data. However, even with a lossy model it is often the case that the result is a
reconstruction that is {\em closer} to the true underlying real-world
values than the original measurements. 
Noise reduction gives rise to a different way of measuring
the quality of a compression scheme. Instead of considering the {\em
amplitude} of the residual using RMS or $\mbox{err}_{\epsilon}$ we
can consider whether or not the residual is white noise. If the models
that we consider are linear we can measure how similar the residual is
to white noise by considering the auto-correlation function for each
residual and the cross-correlation between each pair of residuals. We
expect the auto-correlation to consist of a single delta-function at
zero and we expect the cross-correlation functions to be close to zero
everywhere.

The following section opens a second path towards noise reduction

\iffalse
This is possible when the model used for compression captures some
inherent regularity in the data. Consider a temperature guage in a
room that is taking a measurement ten times per second. Suppose also
that each measurement is the sum of the true temperature and gaussian
noise with a standard deviation of one degree.  It is clear that
replacing blocks of 10 consecutive measurements by their average is
lossy in terms of the original signal. However, at the same time, the
averaged sequence is closer to the true measurements. This noise
reduction relies of the assumption that room temperature rarely
changes significantly within a tenth of a second. Therefor any rapid
variations in the measured temperature is likely to be an artifact of
the sensor rather than anything real.
\fi



\subsection{Better models and higher compression by dependencies}
\label{sec:dependencies}

When the data to be compressed arrives from a large number of
proximate sensors, there are potentially large benefits from
compressing them together using a predictive statistical model such as
ARMA. Unfortunately, the space of models that capture all possible dependencies
between the sensor readings is extremely large and hard to learn fully automatically.
In this case, Plato will engage the model administrator in a semiautomatic procedure.

The role of the model administrator in this context is to restrict the set
of possible interactions to make the compression most effective,
while keeping the associated the learning problem reasonable easy. In the
example of modeling the temperatures of rooms in a building the model
might be restricted to considering the dependence between the outside
temperature and that of the rooms and in addition, the dependence
between neighboring rooms. A model that identifies the stronger
dependencies within these restrictions is useful both for data
compression, as a model of the physical reality (which rooms are
better insulated than others) and as a basis for noise reduction.

Notice that noise reduction is
possible when a good model of the dependencies between different
sensors is available. In such situations the compression and
decompression of the data shifts the data from the measured vector
towards the manifold that is consistent with the physical and
statistical properties of the data. The result is similar to that of
{\em smoothing} the data, with the important difference that the
smoothing is based solely on the statistical model that was inferred from the data.



\subsection{Model Maintenance}
\label{sec:ivm}

When considering a database system that processes data streams over
long periods of time it is necessary to consider how to maintain the
models over time. We divide this problem into two parts, depending on
whether or not the data source is stationary.

When the data source is stationary, efficient model maintenance is
closely related to the subjects of sufficient statistics and or data
synopses or sketches~\cite{}. In any of these formulation the goal is
to define a small data structure that stores the information necessary
to define the model. We plan to test the methods proposed in the
literature and, where necessary, develop new ones.

When the data source is not stationary, we need our model to track the
distribution of the data. This causes an unavoidable tradeoff between
accuracy and adaptivity. In order to adapt quickly the model has to
ignore data from the distant past. At the same time, ignoring data
from the distant past limit the accuracy at which the model parameters
can be estimated. Calibrating the algorithm to consider the optimal
window into the past has been extensively studied in online learning
theory~\cite{Cesa-Bianchi}, in particular
in~\cite{HerbsterWarmuth,Specialists,Warmuth bosquet}. Plato will use
these online learning methods for efficient mode maintenance when the
data distribution varies over time.

\eat{
\reminder{skip this subsection. may become irrelevant due to the increasing depth representation discussed in the query processing}
The model administrator can choose more or less compressed model alternatives. The choice presents a speed-accuracy trade-off: Reduced-noise model alternatives will enable precise query answers but at the cost of speed, since their representations will be relatively large. In contrast, 
lossy model alternatives will lead to very fast queries but at the cost of query answer accuracy. Plato will provide a {\em model administrator assistant} module that semiautomates the process of choosing the appropriate model representation by solving the following problems.

% query answering using a model alternative
% what is the accuracy damage

\paragraph{Choosing a model alternative given a query}
In the simplest setting, the assistant is given
\begin{enumerate} 
%
\item data measurements.
%
\item a model generator that can produce a reduced-noise model $f_{nr}$ of the measurements
%
\item a lossy model generator and candidate loss parameters. For example, the assistant may given the \texttt{fourier\_rms} model generator with candidate RMS error (loss parameter) 0.01, 0.02, 0.03, ..., 0.20. In principle, each setting $e_i$ of the loss parameter leads to another model $f_i$. Furthermore, the representation size of $f_i$ is larger than the representation size of $f_{i+1}$ and all of them are smaller than $f_{nr}$. Let us call 
%
\item a single query $Q$ that uses a model $f$ and a specification of the desired accuracy of the result.
%
\end{enumerate}
}
